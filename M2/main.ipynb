{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "33db5870",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df=pd.read_csv(\"dataset/dataset.csv\")\n",
    "df=df.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "35279c5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df.to_csv(\"./preprocessed/preprocessed2.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e53fe0c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ddd09083",
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "Path does not exist: file:/E:/ongoing/spark/preprocessed/preprocessed2.csv;",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [5]\u001b[0m, in \u001b[0;36m<cell line: 7>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      5\u001b[0m sqlContext \u001b[38;5;241m=\u001b[39m SQLContext(sc)\n\u001b[0;32m      6\u001b[0m spark \u001b[38;5;241m=\u001b[39m SparkSession\u001b[38;5;241m.\u001b[39mbuilder\u001b[38;5;241m.\u001b[39mmaster(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlocal\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mappName(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msample\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mgetOrCreate()\n\u001b[1;32m----> 7\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43msqlContext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcom.databricks.spark.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m(\u001b[49m\u001b[43mheader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrue\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minferschema\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrue\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mE:/ongoing/spark/preprocessed/preprocessed2.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m df\u001b[38;5;241m.\u001b[39mtake(\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m      9\u001b[0m df\u001b[38;5;241m.\u001b[39mshow()\n",
      "File \u001b[1;32mC:\\Spark\\spark-3.0.3-bin-hadoop2.7\\python\\pyspark\\sql\\readwriter.py:178\u001b[0m, in \u001b[0;36mDataFrameReader.load\u001b[1;34m(self, path, format, schema, **options)\u001b[0m\n\u001b[0;32m    176\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions)\n\u001b[0;32m    177\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path, basestring):\n\u001b[1;32m--> 178\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_df(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jreader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    179\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    180\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(path) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlist\u001b[39m:\n",
      "File \u001b[1;32mC:\\Spark\\spark-3.0.3-bin-hadoop2.7\\python\\lib\\py4j-0.10.9-src.zip\\py4j\\java_gateway.py:1304\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1298\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1299\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1300\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1301\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1303\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1304\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1305\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1307\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1308\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[1;32mC:\\Spark\\spark-3.0.3-bin-hadoop2.7\\python\\pyspark\\sql\\utils.py:134\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    130\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[0;32m    131\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[0;32m    132\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[0;32m    133\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[1;32m--> 134\u001b[0m     \u001b[43mraise_from\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconverted\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    135\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    136\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "File \u001b[1;32m<string>:3\u001b[0m, in \u001b[0;36mraise_from\u001b[1;34m(e)\u001b[0m\n",
      "\u001b[1;31mAnalysisException\u001b[0m: Path does not exist: file:/E:/ongoing/spark/preprocessed/preprocessed2.csv;"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql import SparkSession\n",
    "sc = SparkContext.getOrCreate()\n",
    "sqlContext = SQLContext(sc)\n",
    "spark = SparkSession.builder.master(\"local\").appName(\"sample\").getOrCreate()\n",
    "df = sqlContext.read.format('com.databricks.spark.csv').options(header='true', inferschema='true').load('E:/ongoing/spark/preprocessed/preprocessed2.csv')\n",
    "df.take(1)\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "44a33040",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'count'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[1;32mIn [6]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRows= \u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcount\u001b[49m())\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mColumns= \u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;28mlen\u001b[39m(df\u001b[38;5;241m.\u001b[39mcolumns))\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'count'"
     ]
    }
   ],
   "source": [
    "print(\"Rows= \",df.count())\n",
    "print(\"Columns= \",len(df.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "39431ee8",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'select'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[1;32mIn [7]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;124;03m'''check null values'''\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msql\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m isnull, when, count, col\n\u001b[1;32m----> 3\u001b[0m \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m([count(when(isnull(c), c))\u001b[38;5;241m.\u001b[39malias(c) \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m df\u001b[38;5;241m.\u001b[39mcolumns])\u001b[38;5;241m.\u001b[39mshow()\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'select'"
     ]
    }
   ],
   "source": [
    "'''check null values'''\n",
    "from pyspark.sql.functions import isnull, when, count, col\n",
    "df.select([count(when(isnull(c), c)).alias(c) for c in df.columns]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "334df0e4",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'count'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[1;32mIn [8]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcount\u001b[49m(),\u001b[38;5;28mlen\u001b[39m(df\u001b[38;5;241m.\u001b[39mcolumns))\n\u001b[0;32m      2\u001b[0m CONT_COLS_NAMES\u001b[38;5;241m=\u001b[39m[]\n\u001b[0;32m      3\u001b[0m STRING_COLS_NAMES\u001b[38;5;241m=\u001b[39m[]\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'count'"
     ]
    }
   ],
   "source": [
    "print(df.count(),len(df.columns))\n",
    "CONT_COLS_NAMES=[]\n",
    "STRING_COLS_NAMES=[]\n",
    "for i in df.dtypes:\n",
    "    if i[1] == \"string\":\n",
    "        STRING_COLS_NAMES.append(i[0])\n",
    "    else:\n",
    "        CONT_COLS_NAMES.append(i[0])\n",
    "print(STRING_COLS_NAMES)\n",
    "print(CONT_COLS_NAMES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4702fc05",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''label'''\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "stage_1 = StringIndexer(inputCol= 'age', outputCol= 'age_index')\n",
    "stage_1.setHandleInvalid(\"keep\")\n",
    "stage_2 = StringIndexer(inputCol= 'sex', outputCol= 'sex_index')\n",
    "stage_2.setHandleInvalid(\"keep\")\n",
    "stage_3 = StringIndexer(inputCol= 'cp', outputCol= 'cp_index')\n",
    "stage_3.setHandleInvalid(\"keep\")\n",
    "stage_4 = StringIndexer(inputCol= 'trestbps', outputCol= 'trestbps_index')\n",
    "stage_4.setHandleInvalid(\"keep\")\n",
    "stage_5 = StringIndexer(inputCol= 'chol', outputCol= 'chol_index')\n",
    "stage_5.setHandleInvalid(\"keep\")\n",
    "stage_6 = StringIndexer(inputCol= 'fbs', outputCol= 'fbs_index')\n",
    "stage_6.setHandleInvalid(\"keep\")\n",
    "stage_7 = StringIndexer(inputCol= 'restecg', outputCol= 'restecg_index')\n",
    "stage_7.setHandleInvalid(\"keep\")\n",
    "stage_8= StringIndexer(inputCol= 'thalach', outputCol= 'thalach_index')\n",
    "stage_8.setHandleInvalid(\"keep\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8123e83d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import OneHotEncoder, VectorAssembler\n",
    "import os\n",
    "stage_14 = OneHotEncoder(inputCols=[\n",
    "                                stage_1.getOutputCol(),stage_2.getOutputCol(), \n",
    "                                stage_3.getOutputCol(),stage_4.getOutputCol(),\n",
    "                                stage_5.getOutputCol(),stage_6.getOutputCol(),\n",
    "                                stage_7.getOutputCol(),stage_8.getOutputCol(),\n",
    "                                \n",
    "                               \n",
    "                                ],\n",
    "                                outputCols= [\n",
    "                                'age_encoded','sex_encoded',\n",
    "                                'cp_encoded','trestbps_encoded',\n",
    "                                'chol_encoded','fbs_encoded',\n",
    "                                'restecg_encoded','thalach_encoded',\n",
    "                             \n",
    "                                \n",
    "                                ])\n",
    "\n",
    "stage_15 = VectorAssembler(inputCols=[\n",
    "                                'age_encoded','sex_encoded',\n",
    "                                'cp_encoded','trestbps_encoded',\n",
    "                                'chol_encoded','fbs_encoded',\n",
    "                                'restecg_encoded','thalach_encoded',\n",
    "                                'exang','oldpeak','slope','ca','thal'\n",
    "                                ],outputCol='features')\n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "regression_pipeline = Pipeline(stages= [stage_1, stage_2, stage_3, stage_4, stage_5,\n",
    "                                        stage_6,stage_7,stage_8,stage_14,stage_15])\n",
    "ppmodel = regression_pipeline.fit(df)\n",
    "data = ppmodel.transform(df)\n",
    "print(data.select('features').show(1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86427d3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Standard Scaling'''\n",
    "from pyspark.ml.feature import StandardScaler\n",
    "standardscaler=StandardScaler().setInputCol(\"features\").setOutputCol(\"Scaled_features\")\n",
    "scalar_fit=standardscaler.fit(data)\n",
    "data=scalar_fit.transform(data)\n",
    "\n",
    "data.select(\"features\",\"Scaled_features\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf8ec4b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = data.randomSplit([0.8, 0.2], seed=12345)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c1fcd11",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "\n",
    "dt = DecisionTreeClassifier(featuresCol = 'Scaled_features', labelCol = 'target')\n",
    "dtModel = dt.fit(train)\n",
    "dt_predictions = dtModel.transform(test)\n",
    "dt_predictions.select('target', 'prediction', 'probability').show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07ecfddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol='target', \n",
    "    predictionCol='prediction', \n",
    "    metricName='accuracy')\n",
    "accuracy = evaluator.evaluate(dt_predictions)\n",
    "print('Train Accuracy = ', accuracy)\n",
    "DT_SC=accuracy*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b70ad48f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "lr = LogisticRegression(featuresCol = 'Scaled_features', labelCol = 'target', maxIter=10)\n",
    "lrModel = lr.fit(train)\n",
    "lr_predictions = lrModel.transform(test)\n",
    "print(lr_predictions)\n",
    "lr_predictions.select('target', 'prediction', 'probability').show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a0b698d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol='target', \n",
    "    predictionCol='prediction', \n",
    "    metricName='accuracy')\n",
    "accuracy = evaluator.evaluate(lr_predictions)\n",
    "print('Train Accuracy = ', accuracy)\n",
    "LR_SC=accuracy*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df212707",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2ba9458",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "rf = RandomForestClassifier(featuresCol = 'Scaled_features', labelCol = 'target')\n",
    "rfModel = rf.fit(train)\n",
    "rf_predictions = rfModel.transform(test)\n",
    "\n",
    "rf_predictions.select('target', 'prediction', 'probability').show(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfc8ebea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train, test = data.randomSplit([0.8, 0.2], seed=12345)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "689be312",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol='target', \n",
    "    predictionCol='prediction', \n",
    "    metricName='accuracy')\n",
    "accuracy = evaluator.evaluate(rf_predictions)\n",
    "model=accuracy\n",
    "print('Train Accuracy = ', accuracy)\n",
    "RF_SC=accuracy*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb70fe02",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Feature_Importance'''\n",
    "print(rfModel.featureImportances)\n",
    "import pandas as pd\n",
    "\n",
    "def ExtractFeatureImp(featureImp, dataset, featuresCol):\n",
    "    list_extract = []\n",
    "    for i in dataset.schema[featuresCol].metadata[\"ml_attr\"][\"attrs\"]:\n",
    "        list_extract = list_extract + dataset.schema[featuresCol].metadata[\"ml_attr\"][\"attrs\"][i]\n",
    "    varlist = pd.DataFrame(list_extract)\n",
    "    varlist['score'] = varlist['idx'].apply(lambda x: featureImp[x])\n",
    "    return(varlist.sort_values('score', ascending = False))\n",
    "\n",
    "ExtractFeatureImp(rfModel.featureImportances, train, \"features\").head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52ef0e6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''model_testing'''\n",
    "\n",
    "print(test.count(),len(test.columns))\n",
    "rf_predictions.select('target', 'prediction', 'probability').show()\n",
    "\n",
    "a=rf_predictions.select('target', 'prediction', 'probability').toPandas()\n",
    "print(a)\n",
    "a.to_csv(\"./output/RANFOR_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3224f579",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''bias'''\n",
    "predictions = rfModel.transform(train)\n",
    "\n",
    "print(train.count(),len(train.columns))\n",
    "\n",
    "predictions.select('target', 'prediction', 'probability').show()\n",
    "\n",
    "a=predictions.select('target', 'prediction', 'probability').toPandas()\n",
    "print(a)\n",
    "a.to_csv(\"./output/RANFOR_Train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6c9d4f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''confusion matrix'''\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "from pyspark.sql.types import FloatType\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6d6a753",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_and_labels = predictions.select(['prediction','target']).withColumn('target',F.col('target').cast(FloatType())).orderBy('prediction')\n",
    "pred_and_labels=pred_and_labels.select(['prediction','target'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cad78e30",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d6c9656",
   "metadata": {},
   "outputs": [],
   "source": [
    "height = [RF_SC,DT_SC,LR_SC]\n",
    "bars = ( 'RF', 'DT','LR')\n",
    "x_pos = np.arange(len(bars))\n",
    "plt.bar(x_pos, height, color=['#E32227', '#267055', '#050A30'])\n",
    "plt.xticks(x_pos, bars)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d0eaa90",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baa8884a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./model/heart.pkl','wb') as f:\n",
    "    pickle.dump(model,f)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7073eb69",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = pickle.load(open('./model/heart.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73d64e73",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac6ecb5b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
