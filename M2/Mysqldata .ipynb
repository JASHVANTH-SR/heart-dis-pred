{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7ae4b078",
   "metadata": {},
   "source": [
    "## Spark\n",
    "\n",
    "#### PySpark is a Python interface to Apache Spark. Besides allowing us to write Spark applications using the Python Application Programming Interface, it also provides a PySpark shell for interactively analyzing data in a distributed environment. PySpark supports most Spark features such as Spark SQL, DataFrame, Streaming, MLlib (machine learning), Spark Core.\n",
    "\n",
    "### Importing findspark package\n",
    "\n",
    "#### Provide findspark.init() to make pyspark importable as a standard library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "98d8ab45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: findspark in c:\\users\\jashwanth\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (2.0.1)Note: you may need to restart the kernel to use updated packages.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.0.1 -> 23.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install findspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9f257de4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d053a2f6",
   "metadata": {},
   "source": [
    "### Invoking Spark Components\n",
    "\n",
    "#### SparkContext is the entry point for all Spark functionality. Running a Spark application starts a driver program with a main function, which starts the SparkContext. The driver program then executes the operation within the worker node's executor.\n",
    "\n",
    "#### SparkContext uses Py4J to launch a JVM and create a JavaSparkContext. By default PySpark has SparkContext as \"sc\" so creating a new his SparkContext doesn't work.\n",
    "\n",
    "### Configuration initialization\n",
    "\n",
    "#### pyspark.SparkConf is the Main access factor for DataFrame and SQL functionality.\n",
    "\n",
    "### Initialization of Spark Session--SparkSession\n",
    "\n",
    "#### Entry point for programming Spark using the Dataset and DataFrame APIs. In environments where this is pre-built (REPL, Notebook, etc.), use the builder to get an existing session.\n",
    "\n",
    "\n",
    "#### After all these processes we are going to display a dataset which we have collected from external resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26f09ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql import SparkSession\n",
    "sc = SparkContext.getOrCreate()\n",
    "sqlContext = SQLContext(sc)\n",
    "spark = SparkSession.builder.master(\"local\").appName(\"sample\").getOrCreate()\n",
    "df = sqlContext.read.format('com.databricks.spark.csv').options(header='true', inferschema='true').load('C:/Users/JASHWANTH/project/Performance Evaluation of Distributed Machine Learning/M2/preprocessed/preprocessed2.csv')\n",
    "df.take(1)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26ca5c72",
   "metadata": {},
   "source": [
    "### Display Total Number of Rows and Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59208fb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"Rows= \",df.count())\n",
    "print(\"Columns= \",len(df.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "316eccd7",
   "metadata": {},
   "source": [
    "### We have to check any Empty values have been observed in the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1f8f050",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''check null values'''\n",
    "from pyspark.sql.functions import isnull, when, count, col\n",
    "df.select([count(when(isnull(c), c)).alias(c) for c in df.columns]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53ada4f3",
   "metadata": {},
   "source": [
    "### The Column names are Displayed to show the fields used in our Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "050aacf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(df.count(),len(df.columns))\n",
    "CONT_COLS_NAMES=[]\n",
    "STRING_COLS_NAMES=[]\n",
    "for i in df.dtypes:\n",
    "    if i[1] == \"string\":\n",
    "        STRING_COLS_NAMES.append(i[0])\n",
    "    else:\n",
    "        CONT_COLS_NAMES.append(i[0])\n",
    "print(STRING_COLS_NAMES)\n",
    "print(CONT_COLS_NAMES)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b5dd893",
   "metadata": {},
   "source": [
    "### Indexing the labels---StringIndexer\n",
    "\n",
    "#### A label indexer that maps labeled string columns to ML columns with labeled indexes. If the input column is numeric, convert it to a string and index the string value. The index is in [0, numLabels) . By default, this is ordered by label frequency, so the most common label has index 0. Sorting behavior is controlled by the stringOrderType setting. The default value is \"frequencyDesc\".\n",
    "\n",
    "### Handle the InvalidValues---HandleValid\n",
    "\n",
    "#### setHandleInvalid ---> set the value of handleInvalid.\n",
    "\n",
    "#### handleInvalid = Param(parent='undefined', name='handleInvalid', doc=\"How to handle invalid data (invisible or null values) in string type features and label columns. Option is 'skip' (exclude invalid rows data), error (print an error), or 'keep' (put invalid data in a special extra bucket at index numLabels)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40be45dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''label'''\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "stage_1 = StringIndexer(inputCol= 'age', outputCol= 'age_index')\n",
    "stage_1.setHandleInvalid(\"keep\")\n",
    "stage_2 = StringIndexer(inputCol= 'sex', outputCol= 'sex_index')\n",
    "stage_2.setHandleInvalid(\"keep\")\n",
    "stage_3 = StringIndexer(inputCol= 'cp', outputCol= 'cp_index')\n",
    "stage_3.setHandleInvalid(\"keep\")\n",
    "stage_4 = StringIndexer(inputCol= 'trestbps', outputCol= 'trestbps_index')\n",
    "stage_4.setHandleInvalid(\"keep\")\n",
    "stage_5 = StringIndexer(inputCol= 'chol', outputCol= 'chol_index')\n",
    "stage_5.setHandleInvalid(\"keep\")\n",
    "stage_6 = StringIndexer(inputCol= 'fbs', outputCol= 'fbs_index')\n",
    "stage_6.setHandleInvalid(\"keep\")\n",
    "stage_7 = StringIndexer(inputCol= 'restecg', outputCol= 'restecg_index')\n",
    "stage_7.setHandleInvalid(\"keep\")\n",
    "stage_8= StringIndexer(inputCol= 'thalach', outputCol= 'thalach_index')\n",
    "stage_8.setHandleInvalid(\"keep\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97ba4aab",
   "metadata": {},
   "source": [
    "###  Encode into Vector - -OneHotEncoder\n",
    "\n",
    "#### A one-hot encoder that maps columns of category indices to columns of binary vectors. \n",
    "#### Used at most one value per row that specifies the input category index. \n",
    "#### For example, for five categories, an input value of 2.0 maps to an output vector of [0.0, 0.0, 1.0, 0.0]. The last category is not included by default (configurable with dropLast). This is because the vector entries sum to 1 and are linearly dependent. So an input value of 4.0 is equivalent to [0.0, 0.0, 0.0, 0.0].\n",
    "\n",
    "#### If handleInvalid is set to 'keep', 'Category' is added as the last category to indicate invalid values. So if dropLast is true, all invalid values are encoded as a vector of zeros.\n",
    "\n",
    "### Assembling the Vector--VectorAssembler\n",
    "\n",
    "#### A feature transformer that merges multiple columns into a vector column.\n",
    "\n",
    "### Creating Pipeline\n",
    "\n",
    "#### A simple pipeline that acts as an estimator. A pipeline consists of a series of stages, where each stage is either an estimator or a transformer. \n",
    "#### When Pipeline.fit() is called, the phases are executed in order. If the stage is an estimator, the Estimator.fit() method is called with the input dataset to fit the model. Then use a model, a transformer, to transform the dataset as input for the next stage. \n",
    "#### If the stage is a transformer, its Transformer.transform() method is called to create the dataset for the next stage. \n",
    "\n",
    "#### Regression pipelines allow you to predict the value of some numeric attribute for each of your users. If you know the value of that attribute for a subset of users, you can use a Regression pipeline to leverage that information into broad insights about your entire set of users."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aed64061",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.ml.feature import OneHotEncoder, VectorAssembler\n",
    "import os\n",
    "stage_14 = OneHotEncoder(inputCols=[\n",
    "                                stage_1.getOutputCol(),stage_2.getOutputCol(), \n",
    "                                stage_3.getOutputCol(),stage_4.getOutputCol(),\n",
    "                                stage_5.getOutputCol(),stage_6.getOutputCol(),\n",
    "                                stage_7.getOutputCol(),stage_8.getOutputCol(),\n",
    "                                \n",
    "                               \n",
    "                                ],\n",
    "                                outputCols= [\n",
    "                                'age_encoded','sex_encoded',\n",
    "                                'cp_encoded','trestbps_encoded',\n",
    "                                'chol_encoded','fbs_encoded',\n",
    "                                'restecg_encoded','thalach_encoded',\n",
    "                             \n",
    "                                \n",
    "                                ])\n",
    "\n",
    "stage_15 = VectorAssembler(inputCols=[\n",
    "                                'age_encoded','sex_encoded',\n",
    "                                'cp_encoded','trestbps_encoded',\n",
    "                                'chol_encoded','fbs_encoded',\n",
    "                                'restecg_encoded','thalach_encoded',\n",
    "                                'exang','oldpeak','slope','ca','thal'\n",
    "                                ],outputCol='features')\n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "regression_pipeline = Pipeline(stages= [stage_1, stage_2, stage_3, stage_4, stage_5,\n",
    "                                        stage_6,stage_7,stage_8,stage_14,stage_15])\n",
    "ppmodel = regression_pipeline.fit(df)\n",
    "data = ppmodel.transform(df)\n",
    "print(data.select('features').show(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07345705",
   "metadata": {},
   "source": [
    "### Rectification of Errors--Standard Scaler\n",
    "\n",
    "#### Standardize the features by removing the mean and scaling to unit variance using the column summary statistics of the training set samples.\n",
    "\n",
    "#### \"Unit Std\" is calculated using the corrected sample standard deviation, calculated as the square root of the unbiased sample variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d194aee",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Standard Scaling'''\n",
    "from pyspark.ml.feature import StandardScaler\n",
    "standardscaler=StandardScaler().setInputCol(\"features\").setOutputCol(\"Scaled_features\")\n",
    "scalar_fit=standardscaler.fit(data)\n",
    "data=scalar_fit.transform(data)\n",
    "\n",
    "data.select(\"features\",\"Scaled_features\").show(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7299f6e9",
   "metadata": {},
   "source": [
    "### Initiation of Testing and Training\n",
    "\n",
    "#### A random value is set to be the instance case for analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c377467",
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = data.randomSplit([0.8, 0.2], seed=12345)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08758def",
   "metadata": {},
   "source": [
    "### 1st ML classifier Algorithm --> Decision Tree\n",
    "#### Decision trees and their ensembles are common methods for classification and regression machine learning tasks. \n",
    "#### Decision trees are widely used because they are easy to interpret, handle categorical features, scale to multiclass classification settings, do not require feature scaling, and can capture nonlinearities and feature interactions.\n",
    "\n",
    "#### The Decision Tree Pipeline API offers slightly more functionality than the original API. \n",
    "#### In particular, classification allows users to obtain predicted probabilities (also called conditional class probabilities) for each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66eef51f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "\n",
    "dt = DecisionTreeClassifier(featuresCol = 'Scaled_features', labelCol = 'target')\n",
    "dtModel = dt.fit(train)\n",
    "dt_predictions = dtModel.transform(test)\n",
    "dt_predictions.select('target', 'prediction', 'probability').show(10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c46cffa9",
   "metadata": {},
   "source": [
    "###  Multi class Classification - initial Training Accuracy Measurement\n",
    "\n",
    "### MulticlassClassificationEvaluator\n",
    "\n",
    "#### Evaluator for Multiclass Classification, which expects input columns: prediction, label, weight (optional) and probabilityCol (only for logLoss)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b00cfe43",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol='target', \n",
    "    predictionCol='prediction', \n",
    "    metricName='accuracy')\n",
    "accuracy = evaluator.evaluate(dt_predictions)\n",
    "print('Train Accuracy = ', accuracy)\n",
    "DT_SC=accuracy*100\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b634c171",
   "metadata": {},
   "source": [
    "### 2nd ML algorithm for classifier analysis --> Binary Regression\n",
    "\n",
    "#### logistic regression. This class supports multinomial logistic (softmax) and binomial logistic regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88592a28",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "lr = LogisticRegression(featuresCol = 'Scaled_features', labelCol = 'target', maxIter=10)\n",
    "lrModel = lr.fit(train)\n",
    "lr_predictions = lrModel.transform(test)\n",
    "print(lr_predictions)\n",
    "lr_predictions.select('target', 'prediction', 'probability').show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc6b4749",
   "metadata": {},
   "source": [
    "### Accuracy Check --> 2nd Round after Binary Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e27bee7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol='target', \n",
    "    predictionCol='prediction', \n",
    "    metricName='accuracy')\n",
    "accuracy = evaluator.evaluate(lr_predictions)\n",
    "print('Train Accuracy = ', accuracy)\n",
    "LR_SC=accuracy*100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29cb6da8",
   "metadata": {},
   "source": [
    "### Algorithm 3 --> Random Forest Algorithm \n",
    "#### A random forest learning algorithm for classification. It supports both binary and multiclass labels, as well as continuous and categorical features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e95482e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "rf = RandomForestClassifier(featuresCol = 'Scaled_features', labelCol = 'target')\n",
    "rfModel = rf.fit(train)\n",
    "rf_predictions = rfModel.transform(test)\n",
    "\n",
    "rf_predictions.select('target', 'prediction', 'probability').show(8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b3ff9dd",
   "metadata": {},
   "source": [
    "### Accuracy Check -->3rd Round "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f8a5fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol='target', \n",
    "    predictionCol='prediction', \n",
    "    metricName='accuracy')\n",
    "accuracy = evaluator.evaluate(rf_predictions)\n",
    "model=accuracy\n",
    "print('Train Accuracy = ', accuracy)\n",
    "RF_SC=accuracy*100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c311f399",
   "metadata": {},
   "source": [
    "### Feature Engineering\n",
    "\n",
    "#### The feature engineering process should select the minimum required features to create a valid model. \n",
    "#### This is because the more features a model contains, the more complex the model (sparse data) and the more susceptible it is to errors due to deviations. \n",
    "#### A common approach to excluding features is to account for their relative importance to the model, then eliminate weak features or feature combinations, and re-evaluate whether the model performs better during cross-validation. is to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54a529ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Feature_Importance'''\n",
    "print(rfModel.featureImportances)\n",
    "import pandas as pd\n",
    "\n",
    "def ExtractFeatureImp(featureImp, dataset, featuresCol):\n",
    "    list_extract = []\n",
    "    for i in dataset.schema[featuresCol].metadata[\"ml_attr\"][\"attrs\"]:\n",
    "        list_extract = list_extract + dataset.schema[featuresCol].metadata[\"ml_attr\"][\"attrs\"][i]\n",
    "    varlist = pd.DataFrame(list_extract)\n",
    "    varlist['score'] = varlist['idx'].apply(lambda x: featureImp[x])\n",
    "    return(varlist.sort_values('score', ascending = False))\n",
    "\n",
    "ExtractFeatureImp(rfModel.featureImportances, train, \"features\").head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4497bfa3",
   "metadata": {},
   "source": [
    "### Model Testing\n",
    "\n",
    "#### Model testing is called the process of evaluating the performance of a fully trained model on a test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "504de2e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''model_testing'''\n",
    "\n",
    "print(test.count(),len(test.columns))\n",
    "rf_predictions.select('target', 'prediction', 'probability').show()\n",
    "\n",
    "a=rf_predictions.select('target', 'prediction', 'probability').toPandas()\n",
    "print(a)\n",
    "a.to_csv(\"./output/RANFOR_test.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa500921",
   "metadata": {},
   "source": [
    "### Bias Checking and Prediction\n",
    "\n",
    "#### Prediction bias is the difference between the model's apparent prediction error and its actual prediction error. \n",
    "#### Predictive bias can occur when the model contains many independent variables relative to the sample size, or when different sets of independent variables are tested in stepwise procedures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8aa8b42",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "'''bias'''\n",
    "predictions = rfModel.transform(train)\n",
    "\n",
    "print(train.count(),len(train.columns))\n",
    "\n",
    "predictions.select('target', 'prediction', 'probability').show()\n",
    "\n",
    "a=predictions.select('target', 'prediction', 'probability').toPandas()\n",
    "print(a)\n",
    "a.to_csv(\"./output/RANFOR_Train.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a90f6558",
   "metadata": {},
   "source": [
    "### Confusion Matrix\n",
    "\n",
    "#### A multi-class classification evaluator.\n",
    "\n",
    "### Importing Float Datatype module from Pyspark-SQL package\n",
    "\n",
    "#### Inorder to filter and fetch the float data type value, FloatType is imported"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd6b436a",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''confusion matrix'''\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "from pyspark.sql.types import FloatType\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "546be3a5",
   "metadata": {},
   "source": [
    "#### To select the random value predicted with some constraints needed for measurement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7be00955",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_and_labels = predictions.select(['prediction','target']).withColumn('target',F.col('target').cast(FloatType())).orderBy('prediction')\n",
    "pred_and_labels=pred_and_labels.select(['prediction','target'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d8af76c",
   "metadata": {},
   "source": [
    "### Graphical Representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35659743",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8096a15",
   "metadata": {},
   "source": [
    "#### Now we compare the accuracy level of ML algorithms output and based on that we are going to select and execute the process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71e13574",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "height = [RF_SC,DT_SC,LR_SC]\n",
    "bars = ( 'RF', 'DT','LR')\n",
    "x_pos = np.arange(len(bars))\n",
    "plt.bar(x_pos, height, color=['#E32227', '#267055', '#050A30'])\n",
    "plt.xticks(x_pos, bars)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3251dbd",
   "metadata": {},
   "source": [
    "#### It will be stored as pickle file in 2nd Module which is the key factor for our Analysis Program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c304a455",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('./model/heart.pkl','wb') as f:\n",
    "    pickle.dump(model,f)\n",
    "    f.close()\n",
    "model = pickle.load(open('./model/heart.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e244f99",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "942902dd",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f406574e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
